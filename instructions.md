IxD Project 2026 – 30% of Overall grade

Introduction

The “CompuCore” research group at SETU are developing an educational platform where teachers create assignments, students submit their work, and receive feedback through peer review, instructor assessment, and AI-powered evaluation — all while tracking skills development through exemplar-based practice.

The platform is currently in the prototype stage, meaning its core features and user journeys are still being tested and refined. While the main structure of CompuCore exists, many aspects of its interface, interactions, and workflows need further development to ensure a smooth and intuitive user experience. At this stage, nothing is finalised — the goal is to identify pain points, gather real user insights, and propose design improvements that make the platform more effective and user-friendly before moving toward a finished product.

Project challenge

You’ll take on the role of an interaction designer. Over four weeks, you’ll follow a full interaction design process — analysing real user flows, identifying pain points, evaluating the interface using usability heuristics, designing and prototyping solutions in Figma, running user tests to gather feedback, and documenting your entire process in a professional web report.

Why it matters:

By the end, you’ll have a finished, portfolio-ready interaction design project; built from real analysis, testing, and evidence. You’ll think and work like a professional UX designer, developing the mindset to spot problems, test ideas, and create user-centred solutions.

This isn’t just an assignment — it’s practice for how real design teams create better digital experiences.

Timeframe

The project will follow this schedule.

Week

Dates

Focus

Key Activities & Outcomes

Week 0

Feb 27 – Mar 5

Project Setup & Orientation

Access and explore the STARS app. Review project brief and requirements. Set up your initial project web page with a title, navigation, and placeholders for upcoming sections. Become familiar with the overall workflow and deliverables.

Week 1

Mar 6 – Mar 12

User & Task Analysis

Explore assigned user flows, identify main tasks and pain points, complete task analysis and pain point logs, and refine your web report scaffold.

Week 2

Mar 13 – Mar 19

Interface Evaluation (Heuristic Review)

Conduct a heuristic evaluation using Nielsen’s principles. Annotate screenshots, document usability issues and strengths, and identify your top three usability problems.

Week 3

Mar 20 – Mar 26

Design & Prototyping

Develop mid-fidelity wireframes and a clickable prototype in Figma. Write design proposals explaining how each change addresses specific usability issues.

Week 4

Apr 13 – Apr 17

User Testing & Final Report

Run usability tests with peers, document feedback, reflect on findings, polish all report sections, and submit your final web report and Figma prototype link.

IxD Project — Week 1

User & Task Analysis

compucore3.setu.ie/stars

This week

Lab (2 hrs): Walk through your flows as a user. Fill in the task analysis and pain points templates.

Home (2 hrs): Refine your analysis. Scaffold your web report with all 7 sections.

Milestone: Task analysis complete. Web page with working nav and placeholder sections submitted.

1. Your Assigned Flows

Each student is assigned two different user-flows to analyze. Find Your here:
Project 2026 User flows assignment

2. Lab Session (2 Hours)

Part 1 — Do the Flows (45 min)

Before analysing anything, just use the app. Complete each of your two flows from start to finish — at least twice each.

app web page: compucore3.setu.ie/stars

First pass: Just do it. Don't take notes yet.

Second pass: Think aloud quietly as you go. Say what you expect to happen, what confused you, where you hesitated.

Think aloud prompts

"I expected this button to... but it..."

"I'm not sure if this saved or not"

"I don't know what this term means"

"I had to re-read this to understand it"

"I accidentally did X when I meant to do Y"

Part 2 — Task Analysis Template (40 min)

Complete one table per flow. Aim for 4–8 steps. A step = one meaningful action or decision, not a single click. Note these tables will be added to your web page as a HTML table

Flow 1 — Overview

Flow name

User role

Student / Teacher

Trigger (why does the user start this?)

End state (what does success look like?)

#

User Action

System Response

User's Goal

Pain Point / Issue?

1

2

3

4

5

6

7

8

9

10

Flow 2 — Overview

Flow name

User role

Student / Teacher

Trigger (why does the user start this?)

End state (what does success look like?)

#

User Action

System Response

User's Goal

Pain Point / Issue?

1

2

3

4

5

6

7

8

9

10

Part 3 — Pain Point Log (35 min)

This is the most important part of Week 1. For every moment of confusion, hesitation, or error you noticed during the walkthrough, record it here. You do not need to diagnose the problem yet — just capture it clearly.

What counts as a pain point?

You hesitated or had to re-read something before acting

You clicked something and got an unexpected result

You were unsure whether an action had worked

You couldn't find what you were looking for

You made an error and had to recover

You felt uncertain about what a term or label meant

You were unsure what to do next

If you found no pain points — you were not looking hard enough.

Even well-designed interfaces have friction. Try a harder test: imagine a first-year student using this for the first time.

#

Flow

Where in the flow? (step/screen)

What happened?

Why is this a problem?

1

2

3

4

5

6

7

8

9

10

Severity rating — add to each row once you have 3+ entries

High: blocks the user or causes data loss

Medium: causes confusion or extra steps

Low: minor annoyance, cosmetic issue

3. Home Work (2 Hours)

Task 1 — Tidy Up (45 min)

Review your step tables — fill any gaps, make sure each row tells a clear story

Add severity ratings (High / Medium / Low) to every pain point

Write 2–3 sentences summarising each flow: who does it, when, and what makes it work or not work

Prioritise your top 3 pain points overall — these will drive your design proposals in Week 3

Task 2 — Web Report Scaffold (75 min)

Build a single web page with a working navigation menu. It will grow each week — keep the code clean and easy to edit.

#

Page

This Week

1

Introduction

Write fully — app name, your name, your two flows, why they matter

2

User & Task Analysis

Embed your completed tables and pain point log

3

Interface Evaluation

Placeholder heading only

4

Design Proposals

Placeholder heading only

5

Prototype

Placeholder heading only

6

User Test

Placeholder heading only

7

Reflection

Placeholder heading only

4. Week 1 Checklist

Done

Item

☐

Completed both flows at least twice (think aloud on second pass)

☐

Flow Overview filled in for both flows

☐

Step table completed for both flows (6–10 steps each)

☐

At least 5 pain points logged with severity ratings

☐

Top 3 pain points identified and noted

☐

2–3 sentence summary written for each flow

☐

Web page created with working nav menu

☐

All 7 sections present (placeholders OK for 3–7)

☐

Introduction section fully written

☐

Task analysis and pain point tables embedded in Section 2

IxD Project — Week 2

Interface Evaluation

compucore3.setu.ie/stars

This week

Lab (2 hrs): Heuristic walkthrough of your two flows. Annotate screenshots. Fill in the findings table.

Home (2 hrs): Write the Evaluation section of your web report. Begin sketching design ideas.

Milestone: Evaluation section complete with annotated screenshots and a structured findings table.

1. What Is a Heuristic Evaluation?

A heuristic evaluation is a structured method for finding usability problems. You walk through the interface and judge each step against a set of established principles — the heuristics — asking: does this design follow this principle, or does it violate it?

This week you are combining two things you already have from Week 1:

Your task analysis — the steps you mapped out

Your pain points — moments of friction you noticed

Now you are going to explain those pain points using the heuristics, and find any new ones you may have missed.

The 10 Heuristics (Nielsen)

Use these as a lens when walking through each step of your flows. You do not need to apply all 10 — focus on the ones most relevant to what you observe.

ID

Heuristic

Ask yourself...

H1

Visibility of system status

Does the app always show what is happening? (loading, saved, submitted)

H2

Match between system & real world

Does it use language the user understands? Are concepts familiar?

H3

User control & freedom

Can users undo, go back, or cancel easily?

H4

Consistency & standards

Do similar things look and behave the same way throughout?

H5

Error prevention

Does the interface stop users making mistakes before they happen?

H6

Recognition over recall

Can users see options rather than having to remember them?

H7

Flexibility & efficiency

Can experienced users take shortcuts?

H8

Aesthetic & minimalist design

Is there clutter or unnecessary information on screen?

H9

Error recovery

Are error messages clear? Do they help the user fix the problem?

H10

Help & documentation

Are instructions available when the user is stuck?

2. Lab Session (2 Hours)

Part 1 — Heuristic Walkthrough (60 min)

Go through each flow step by step. At each step, ask which heuristics apply and whether the interface satisfies or violates them.

Take screenshots as you go. Capture the screen at every step where you find a problem or a strength. You will annotate these later.

Screenshot tips

Windows: Win + Shift + S | Mac: Cmd + Shift + 4 | Browser: right-click → Save image

Name files clearly: flow1_step3_no_confirmation.png

Aim for at least 2–3 annotated screenshots per flow

Annotations can be arrows, circles, or callout boxes — use any image editor or Figma

For each issue or strength, ask three things:

Which heuristic? Use the H1–H10 codes from the reference card

What exactly happens? Describe the specific behaviour, not just a vague label

Why does it matter? What is the impact on the user — confusion, error, wasted time?

Example

Step: User submits artefact. No confirmation message appears.

Heuristic violated: H1 — Visibility of system status

What happens: After clicking Submit, the page reloads silently. Nothing confirms the submission went through.

Why it matters: The user cannot tell if it worked. They may submit again, or leave thinking it failed.

Part 2 — Findings Table (60 min)

Record every finding — both problems and things that work well. Aim for at least 6 findings across your two flows. Every finding needs a screenshot reference.

#

Flow

Step

H#

Strength or Problem?

What happens

Impact on user

Screenshot ref

1

2

3

4

5

6

7

8

9

10

11

12

Severity ratings — add a column or note inline

High: blocks completion of the flow, or likely to cause errors

Medium: causes confusion or extra steps, but user can recover

Low: cosmetic, minor annoyance

Don't just log problems

Genuine strengths are valuable too — they tell you what to preserve when redesigning.

Aim for roughly 70% problems, 30% strengths.

3. Home Work (2 Hours)

Task 1 — Annotate Screenshots (30 min)

Take your screenshots and annotate the key issues. Each annotation should:

Point to the specific UI element with an arrow or circle

Include a short label (e.g. "No confirmation — H1")

Be readable at the size you will embed it in your web report

Tools: Figma (free), Preview (Mac), Paint / Snipping Tool (Windows), or any image editor.

Task 2 — Write the Evaluation Section in your web page (60 min)

Add the Interface Evaluation section to your web report. It should contain:

Sub-section

What to include

Method

1–2 sentences: what a heuristic evaluation is and how you conducted yours

Findings table

Your completed table from the lab, embedded or formatted clearly

Annotated screenshots

At least 2 per flow, with captions referencing the heuristic

Summary

3–5 sentences: overall verdict — what works, what doesn't, and the top 3 issues that most need fixing

The summary is the most important part

Name your top 3 issues explicitly. These will become the starting point for your design proposals in Week 3.

Link them back to your Week 1 pain points — do they match? Any surprises?

Task 3 — Begin Sketching (30 min)

Start thinking about how you would fix the top 3 issues. Do rough sketches — paper is fine, photos of paper are fine.

One sketch per issue is enough at this stage

Focus on the idea, not the drawing quality

Bring your sketches to the Week 3 lab — you will develop them into wireframes

What makes a useful sketch?

It shows what is on screen — not just a box labelled "better UI"

It addresses the specific heuristic violation — e.g. adds a confirmation message for H1

It is rough enough to change — don't commit to a direction yet

4. Week 2 Checklist

Done

Item

☐

Heuristic walkthrough completed for both flows

☐

At least 10 findings logged in the table (mix of problems and strengths)

☐

Every finding has a heuristic code (H1–H10) and a severity rating

☐

At least 2 annotated screenshots per flow (4+ total)

☐

Evaluation section added to web report with all four sub-sections

☐

Summary names the top 3 issues explicitly

☐

Top 3 issues linked back to Week 1 pain points

☐

At least 3 rough sketches produced (one per top issue)

☐

Sketches photographed or scanned, ready to bring to Week 3 lab

IxD Project — Week 3

Design & Prototype

compucore3.setu.ie/stars

This week

Lab (2 hrs): Share sketches with a partner for feedback. Refine into mid-fi wireframes in Figma. Build a clickable prototype covering both flows.

Home (2 hrs): Finish the prototype. Write the Design Proposals section of your web report.

Milestone: Clickable prototype covering both flows. Design Proposals section drafted in web report.

1. Before You Start

You should be arriving at the lab with:

Your top 3 issues from Week 2 (named explicitly in your evaluation summary)

At least 3 rough sketches — one idea per issue — on paper or photographed

Access to Figma (free account at figma.com — set this up before the lab if you haven't already)

No sketches? Spend the first 15 minutes of the lab doing them now — on paper, quickly.

The point of sketches is speed. A sketch takes 5 minutes. A Figma frame takes 30.

Do not go straight to Figma. Sketches first.

2. Lab Session (2 Hours)

Part 1 — Peer Sketch Review (30 min)

Pair up with someone who has different assigned flows to yours. You will review each other's sketches using the form at the end of this document.

Designer (15 min): Walk your reviewer through each sketch. Explain the problem you are solving and how the sketch addresses it. Do not defend — just explain.

Reviewer (15 min): Fill in the peer feedback form. Be specific. "Good" is not feedback. "The confirmation message idea solves H1 clearly, but I can't tell where it appears on screen" is feedback.

What you are looking for in feedback

Is the solution clearly visualised — or is it still vague?

Does it actually fix the heuristic problem, or does it just move it?

Are there simpler or more direct ways to solve it?

Part 2 — Mid-Fi Wireframes in Figma (90 min)

Using your sketches and peer feedback, build mid-fidelity wireframes in Figma. Mid-fi means: real layout, real labels, real interactions — but no colour, no photography, no final styling.

Level

What it includes

What it leaves out

Lo-fi (sketch)

Shape and flow only

Labels, real content, interactions

Mid-fi (this week)

Layout, labels, navigation, interactions

Colour, imagery, final typography

Hi-fi

Visual design, brand, assets

Nothing — production-ready

Figma Setup

Create a new Figma file. Name it: IxD-[YourName]-[PairNumber]

Create a frame for each screen in your flows: use Frame > Phone (390 × 844) or Desktop (1440 × 900) — whichever matches how the app is used

Name every frame clearly: e.g. Flow1_Step2_SubmitPage

Group your frames by flow so the structure is clear

What Your Prototype Must Cover

Your prototype needs to let a tester walk through both of your assigned flows from start to finish without getting stuck. Every flow needs:

Required element

Why it matters for testing

A clear starting screen

Tester needs to know where the flow begins

Every step that involves a decision or input

Skipping steps means the tester can't complete the flow

Feedback states (success, error, loading)

These are often where the biggest issues are — don't skip them

A clear end state

Tester must be able to tell when the flow is complete

Clickable hotspots on all interactive elements

Dead clicks break the test — link every button and nav item

Figma prototype connections

Select a frame → click Prototype tab → drag from element to destination frame

Use 'On Click → Navigate to' for most interactions

Use 'On Click → Open overlay' for modals and confirmation dialogs

Test your own prototype before home time: click through the whole flow once

Scope warning

You do not need to redesign the whole app — only the screens involved in your two flows.

You do not need to fix every problem — focus your top 3 issues from Week 2.

A prototype that covers 2 flows cleanly is better than one that half-covers 5.

3. Home Work (2 Hours)

Task 1 — Finish the Prototype (60 min)

Complete any frames you didn't finish in the lab

Walk through both flows yourself from start to finish in Figma's present mode

Fix any dead links, missing states, or frames that are hard to read

Get the shareable link: Share → Anyone with the link → can view → Copy link

Paste the link into the Prototype section of your web report

Task 2 — Write the Design Proposals Section (60 min)

Add a Design Proposals section to your web report. Use the proposal template below as your structure — one block per proposal, minimum three proposals.

What makes a good proposal?

It names the specific problem (heuristic + description from Week 2)

It describes what changed on screen — not just 'better labelling' but 'the Submit button now shows a spinner and then a green tick with the text Your work has been submitted'

It explains why the change fixes the problem — connect it back to the heuristic

It is honest about trade-offs — does the change add complexity elsewhere?

Complete one block like this for each of your design proposals:

Design Proposal 1

Problem being addressed

(link to your top issues from Week 2 — name the heuristic)

What you changed

(describe the new design — be specific about what appears on screen)

Why this helps

(explain how it fixes the heuristic violation or reduces the pain point)

Trade-offs / risks

(anything this change might break or complicate)

Prototype sketch

(name or number of the Figma frame this proposal relates to)

Peer Feedback Form x 2

Prototype screenshot and link

(name or number of the Figma frame this proposal relates to)

Design Proposal 2

Problem being addressed

(link to your top issues from Week 2 — name the heuristic)

What you changed

(describe the new design — be specific about what appears on screen)

Why this helps

(explain how it fixes the heuristic violation or reduces the pain point)

Trade-offs / risks

(anything this change might break or complicate)

Prototype sketch(name or number of the Figma frame this proposal relates to)

Peer Feedback Form x 2

Prototype screenshot and link

(name or number of the Figma frame this proposal relates to)

Design Proposal 3

Problem being addressed

(link to your top issues from Week 2 — name the heuristic)

What you changed

(describe the new design — be specific about what appears on screen)

Why this helps

(explain how it fixes the heuristic violation or reduces the pain point)

Trade-offs / risks

(anything this change might break or complicate)

Prototype sketch(name or number of the Figma frame this proposal relates to)

Peer Feedback Form x 2

Prototype screenshot and link

(name or number of the Figma frame this proposal relates to)

Connecting prototype to proposals

Every proposal must reference at least one screen in your prototype.

In your web report, embed or link a screenshot of the relevant Figma frame next to each proposal.

The reader should be able to see the before (Week 2 annotated screenshot) and the after (your Figma frame) side by side.

4. Week 3 Checklist

Done

Item

☐

Sketches brought to lab (paper or photo)

☐

Peer review completed — feedback form filled in (both sides)

☐

Figma file created with frames named clearly by flow and step

☐

All screens in both flows have a corresponding frame

☐

Feedback / error / success states included where relevant

☐

All interactive elements have clickable prototype connections

☐

Both flows testable end-to-end in Figma present mode

☐

Shareable Figma link embedded in web report Prototype section

☐

Minimum 3 design proposals written up with full rationale

☐

Each proposal references a specific Figma frame

☐

Before/after evidence in web report (Week 2 screenshot + Figma frame)

Peer Feedback Form — Reviewer name: ************\_************

Designer being reviewed: ************\_************ Flow(s): ************\_************

Does the sketch clearly show what is on screen?

Yes / No / Partly

Which design change is most likely to help a real user?

Is there anything unclear or missing from the sketch?

One suggestion to strengthen the strongest idea

IxD Project — Week 3 | Assessment Portfolio | SETU

IxD Project — Week 4

User Testing & Final Report

compucore3.setu.ie/stars

This week

Lab (2 hrs): Run a user test on your partner's prototype (swap flows). Observe, take notes, debrief.

Home (2 hrs): Write up the User Test section. Polish all sections. Submit your web report link.

Milestone: Complete web report submitted — all 7 sections navigable, prototype linked, test documented.

1. Before the Lab

Arrive with the following ready:

Your Figma prototype — both flows testable end-to-end in present mode

Your Figma shareable link copied and ready to paste

Your top 3 issues from Week 2 in mind — you will check whether the test confirms or surprises them

Your task prompts written up (see Section 2 below) — do this before the lab

Write your task prompts before the lab. You cannot run a test without them.

A task prompt is a goal, not a set of instructions. See the script template in this document.

2. What You Are Doing and Why

A usability test puts a real person in front of your prototype and watches what happens. The goal is not to prove your design works — it is to find out where it breaks.

You are running a simple think-aloud test:

Think-aloud: the tester narrates what they see, expect, and do as they go

Task-based: you give them a goal to achieve, not step-by-step instructions

Observed: you watch and take notes — you do not guide, explain, or help

Role

What you do

What you do NOT do

Facilitator (you)

Read the script. Give the task prompt. Watch and take notes. Ask debrief questions.

Explain the interface. Answer 'is this right?'. Help when they hesitate. React to errors.

Tester (your partner)

Attempt the tasks. Think aloud. Complete the debrief.

Follow step-by-step instructions. Pretend to understand something they don't.

The most important rule

Silence is data.

When your tester hesitates, stares, or goes quiet — do not fill the silence.

That pause is them experiencing confusion. It is exactly what you need to observe.

Only intervene if they have been completely stuck for more than 60 seconds.

3. Lab Session (2 Hours)

Part 1 — Swap Prototypes and Prepare (15 min)

Pair up with someone from the class — preferably someone with different assigned flows

Share your Figma prototype link with your partner

Each of you: open your partner's prototype in present mode and navigate to the first screen of Flow 1

Facilitator: read through your task script quietly and make sure your prompts are written for your partner's flows — not your own

You are testing your partner's prototype, not yours

You are the facilitator for their prototype. They are the facilitator for yours.

Each person both runs a test AND sits as a tester.

You will each fill in a separate observation log and findings summary.

Part 2 — Run the User Test (60 min)

Each test takes approximately 25–30 minutes. Both tests should fit within the 60 minutes.

Test structure:

Phase

Duration

What happens

Consent & intro

3 min

Read intro aloud. Tester signs consent form.

Task 1 — Flow 1

10 min

Read task prompt. Observe. Take notes. Do not intervene.

Task 2 — Flow 2

10 min

Read task prompt. Observe. Take notes. Do not intervene.

Debrief

5 min

Ask the three debrief questions. Write down answers verbatim.

Switch roles

—

Swap. Now you are the tester, your partner is the facilitator.

Use the Test Session Script and Observation Log in this document (pages 2–3). Fill them in during the test — do not rely on memory.

Part 3 — Debrief Together (15 min)

After both tests, spend 15 minutes talking through what you each found — not to compare designs, but to sanity-check your observations.

Did your tester confirm the issues you expected, or were there surprises?

What was the single most revealing moment in your test?

Is there anything in your design you would change right now based on what you saw?

Fill in the Findings Summary form (page 4) during this debrief while it is fresh.

4. Home Work (2 Hours)

Task 1 — Write the User Test Section (60 min)

Add the User Test section to your web report. It should contain four parts:

Sub-section

What to include

Method

2–3 sentences: what kind of test you ran, who the participant was (role, not name), what flows they tested

Task prompts

The exact prompts you read aloud — word for word

Findings

Your completed observation log (table or prose summary) + the findings summary. Include at least one direct quote from the tester.

Reflection

Did the test confirm your Week 2 issues? What surprised you? What would you change in the design now? (5–8 sentences)

Quoting your tester

Direct quotes are the most powerful evidence in a user test write-up.

Example: 'I didn't realise it had submitted — I thought I had to click something else.'

One good quote per issue is enough. Use them to support your findings, not replace them.

Task 2 — Write the Reflection Section (20 min)

The Reflection section is the last section of your report. Keep it honest and specific — vague statements like 'I learned a lot' carry no marks.

Cover these four things:

What was the most useful thing you discovered across the whole 4-week process?

Where did your heuristic evaluation and your user test agree — and where did they differ?

If you had one more week, what single change would you make to your design and why?

What would you do differently if you ran this process again from Week 1?

Task 3 — Polish and Submit (40 min)

Work through the Final Report Checklist below. Every item must be ticked before you submit.

Submission: Paste the URL of your web report into the submission form on Moodle. Make sure the link is publicly accessible before submitting.

Test your link before submitting

Open your report URL in a private/incognito browser window.

Check every nav link works. Check the Figma prototype link opens. Check all images load.

If anything is broken in an incognito window, it will be broken for your marker.

5. Final Report Checklist

Done

Section

Must include

☐

Introduction

App described, your name and flows stated, why these flows matter

☐

User & Task Analysis

Both flow overviews complete, step tables filled in, pain point log with severity ratings, top 3 identified

☐

Interface Evaluation

Method noted, findings table with H# codes and severity, 4+ annotated screenshots, summary names top 3 issues

☐

Design Proposals

3+ proposals, each with problem, change, rationale, trade-offs, Figma frame reference

☐

Prototype

Figma link works, both flows testable end-to-end, before/after screenshots alongside proposals

☐

User Test

Method, task prompts, observation log, findings summary, reflection on whether issues were confirmed

☐

Reflection

Honest and specific — what you learned, what you would do differently, one thing you would change about your design now

☐

Navigation

All 7 sections reachable from the nav menu, nav works on a laptop screen without horizontal scrolling

☐

Readability

Headings used consistently, tables readable, screenshots have captions, no broken links

6. Marking Summary

Your final report is marked across five criteria. The User Test and Reflection sections are new this week — the others should already be substantially complete.

Section

Weight

Key question the marker asks

User & Task Analysis

20%

Are the flows mapped accurately with genuine pain points identified?

Interface Evaluation

25%

Are findings grounded in heuristics with clear evidence?

Design Proposals

25%

Are proposals specific, justified, and linked to identified problems?

User Test

20%

Was the test conducted properly? Are findings honestly reported?

Reflection

10%

Is the reflection specific, critical, and honest?

Session Materials — print or photograph before the lab

— Consent Form — one per participant —

Participant Consent Form

Project: IxD Evaluation of Assessment Portfolio App Date: ******\_\_\_******

Participant name: **************\_\_\_************** Student no: ******\_\_\_******

I confirm that:

I understand that this is a student usability test of a prototype interface.

I understand that it is the prototype being tested — not me.

I agree that the session may be observed and notes taken.

I can stop at any time without explanation.

My name will not appear in any public report.

Signature: **************\_\_\_************** Date: ******\_\_\_******

— Test Session Script — facilitator copy —

Test Session Script — Facilitator copy

Introduction (read aloud)

"Thanks for doing this. I'm testing a prototype I designed — not your abilities. There are no wrong answers. Please think aloud as you go: say what you're looking at, what you expect to happen, and what surprises you. I won't help you unless you're completely stuck — that silence is useful data for me."

Task 1 — Flow 1 (read aloud)

Write your task prompt here. Frame it as a goal, not an instruction. Example: 'Imagine you have just finished a piece of work and you want to submit it for feedback. Show me how you would do that.' Do NOT say: 'Click the Submit button.'

During Flow 1 — observer notes

Use the Observation Log on the next page. Note hesitations, errors, comments, facial reactions. Do not intervene unless completely stuck (>60 seconds).

Transition (read aloud)

"Great. Now I'd like you to try something different."

Task 2 — Flow 2 (read aloud)

Write your second task prompt here. Same principle — goal-framed, not step-by-step.

During Flow 2 — observer notes

Continue in the Observation Log.

Debrief (read aloud)

"That's everything. Thank you. I have a few quick questions:" • What was the most confusing moment? • Was there anything that worked exactly as you expected? • If you could change one thing, what would it be?

Debrief notes

(write tester's answers here)

— Observation Log — fill in during the test —

Tester name: **************\_\_\_************** Your name: **************\_\_\_************** Date: ******\_\_\_******

#

Task / Step

What the tester did

Said or expressed

Pass / Struggle / Fail

1

2

3

4

5

6

7

8

9

10

Pass = completed without hesitation Struggle = hesitated or needed a second attempt Fail = could not complete or gave up

— Findings Summary — fill in during debrief —

Post-Test Findings Summary

What did the tester complete successfully?

Where did they hesitate or struggle?

Where did they fail or give up?

Most useful thing they said during think-aloud

Most useful thing they said during debrief

Did your top 3 issues from Week 2 show up?

Yes / Partly / No — explain:

Did any NEW issues appear that you hadn't identified?

Does this change your design? If so, how?

IxD Project — Week 4 | Assessment Portfolio | SETU
